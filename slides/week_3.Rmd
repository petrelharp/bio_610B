---
title: "Simulation, power, and logistic regression"
author: "Peter Ralph"
date: "22 January 2018 -- Advanced Biological Statistics"
---


```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(tidyverse)
library(rstan)
options(mc.cores = parallel::detectCores())
```

## Outline

1. Uses of simulation

2. ... applied to the hierarchical model.

3. Posterior predictive sampling.

4. Shrinkage: sharing information

5. Predicting binary outcomes from quantitative variables: Bayesian logistic regression



# Back to baseball

## From last time:


:::::::::::::: {.columns}
::: {.column width="60%"}

$$\begin{aligned}
    Z_i &\sim \Binom(N_i, \theta_i) \\
    \theta_i &\sim \Beta(\mu_{p_i} \kappa_{p_i}, (1-\mu_{p_i})\kappa_{p_i}) \\
    \mu &\sim \Beta(1, 1) \\
    \kappa_p &\sim \Gam(0.1, 0.1) .
\end{aligned}$$

::::::::::::::
::: {.column width="40%"}


```{r stan_model}
pos_model <- "
data {
    int N;   // number of players
    int hits[N];
    int at_bats[N];
    int npos; // number of positions
    int position[N];
}
parameters {
    real<lower=0, upper=1> theta[N];
    real<lower=0, upper=1> mu[npos];
    real<lower=0> kappa[npos];
}
model {
    real alpha;
    real beta;
    hits ~ binomial(at_bats, theta);
    for (i in 1:N) {
        alpha = mu[position[i]] * kappa[position[i]];
        beta = (1 - mu[position[i]]) * kappa[position[i]];
        theta[i] ~ beta(alpha, beta);
    }
    mu ~ beta(1,1);
    kappa ~ gamma(0.1,0.1);
} "
```


::::::
::::::::::::::


--------------------


```{r fit_pos_model, cache=TRUE}
batting <- read.csv("BattingAverage.csv", header=TRUE)
system.time(pos_fit <- stan(model_code=pos_model, chains=3, iter=100,
                            data=list(N=nrow(batting),
                                      hits=batting$Hits,
                                      at_bats=batting$AtBats,
                                      npos=nlevels(batting$PriPos),
                                      position=as.numeric(batting$PriPos))))
```

## Diagnostics

```{r print_fit}
print(pos_fit, pars=c("mu", "kappa"))
```

----------------

Is it mixing?

```{r plot_trace}
stan_trace(pos_fit, pars="mu")
```


## Run longer!

```{r fit_pos_model_again, cache=TRUE}
system.time(pos_fit <- stan(model_code=pos_model, chains=3, iter=1000,
                            control=list(max_treedepth=15),
                            data=list(N=nrow(batting),
                                      hits=batting$Hits,
                                      at_bats=batting$AtBats,
                                      npos=nlevels(batting$PriPos),
                                      position=as.numeric(batting$PriPos))))
```

-------------

```{r print_fit_again}
print(pos_fit, pars=c("mu", "kappa"))
```


-------------

Is it mixing?

```{r plot_trace_again}
stan_trace(pos_fit, pars="mu")
```

-------------

```{r plot_kappa_again}
stan_trace(pos_fit, pars="kappa")
```

## Let's look at the results!

```{r first_hist}
stan_hist(pos_fit, pars="mu", bins=30) + xlim(0, 0.4)
```

-----------

I like labels.

```{r plot_mu}
param_samples <- extract(pos_fit)
layout(matrix(1:9, ncol=3))
par(mar=c(4,3,1,1)+.1)
for (k in 1:9) {
    hist(param_samples$mu[,k], main=levels(batting$PriPos)[k],
         xlim=c(0, 0.4), xlab='batting average', ylab=expression(mu))
}
```

-----------


```{r plot_kappa}
layout(matrix(1:9, ncol=3))
par(mar=c(4,3,1,1)+.1)
for (k in 1:9) {
    hist(param_samples$kappa[,k], main=levels(batting$PriPos)[k],
         xlab='batting average', ylab=expression(kappa))
}
```

## What else do you want to know?

About
$$\begin{aligned}
    \theta_i &: \text{individual $i$ batting averages} \\
    \mu &: \text{position $p$ mean batting averages} \\
    \kappa_p &: \text{position $p$ variability of batting averages}
\end{aligned}$$


## Do positions with more at-bats have higher batting averages?

To answer this, let's find the posterior distribution
of the difference in means ($\mu_p$) between two positions,
1st base, who have a lot of at-bats, and pitchers, who don't.

To do this, we'll use the random samples obtained with `extract()`.
```{r diff_means}
first_base_col <- which(levels(batting$PriPos) == "1st Base")
pitcher_col <- which(levels(batting$PriPos) == "Pitcher")
mu_diff <- param_samples$mu[,first_base_col] - param_samples$mu[,pitcher_col]
```
The posterior mean of $\mu_\text{1st base} - \mu_\text{pitcher}$ is `r mean(mu_diff)`;
and the posterior SD is `r sd(mu_diff)`.
This is a very large difference: it seems that 1st basemen (on average) in this dataset
are much better batters than pitchers.
We are very certain of this, because none of the `r length(mu_diff)` samples
had pitchers with higher $\mu$ than 1st basemen

## What position has the most consistent batting average?

Find the posterior distribution of SDs of $\theta_i$s, separately by position.

```{r pos_sds}
library(magrittr)
library(dplyr)
pos_sds <- sapply(1:nlevels(batting$PriPos), function (k) {
                      use_these <- batting$PriPos == levels(batting$PriPos)[k]
                      rowSds(param_samples$theta[,use_these])
         } )
colnames(pos_sds) <- levels(batting$PriPos)
boxplot(pos_sds, las=3)
```


# Stochastic minute

## Exponential, and Gamma

If $T \sim \Exp(\text{rate}=\lambda)$, then

$$\begin{aligned}
   \P\{ T \in dt \} = \lambda e^{-\lambda t} dt .
\end{aligned}$$

1. $T$ can be any nonnegative real number.

2. $T$ is *memoryless*: 
   $$\begin{aligned}
        \P\{ T > x + y \given T > x \} = \P\{ T > y \} .
   \end{aligned}$$

3. A machine produces $n$ widgets per second;
   each widget has probability $\lambda/n$ of being broken.
   The time until the first broken widget appears (in seconds)
   is approximately $\sim \Exp(\lambda)$.

---------------------

If $S \sim \Gam(\text{shape}=\alpha, \text{rate}=\lambda)$, then

$$\begin{aligned}
   \P\{ S \in dt \} = \frac{\alpha^\lambda}{\Gam(\alpha)} t^{\alpha - 1} e^{-\lambda t} dt .
\end{aligned}$$

1. If $T_1, \ldots, T_k$ are independent $\Exp(\lambda)$, then
   $S = T_1 + \cdots + T_k$ is $\Gam(k, \lambda)$.


2. A machine produces $n$ widgets per second;
   each widget has probability $\lambda/n$ of being broken.
   The time until the $k^\text{th}$ broken widget appears (in seconds)
   is approximately $\sim \Gam(k, \lambda)$.



# Simulation

## Wouldn't it be nice if we knew the truth?

. . .

**Discuss:**

Write down (on whiteboards)
a procedure to simulate data
that looks like the baseball data.

. . .

**(demonstration)**


------------

Using the posterior mean on $\theta$:
```{r simulate_fake_data}
batting$post_mean <- colMeans(param_samples$theta)
batting$fake_hits <- rbinom(nrow(batting), 
                            size=batting$AtBats, 
                            prob=batting$post_mean)
```

------------

Generating fake data by first sampling $\theta$s:
We'll start with the posterior mean values for $\mu$ and $\kappa$:
```{r simulate_with_thetas}
post_mean_mu <- colMeans(param_samples$mu)
post_mean_kappa <- colMeans(param_samples$kappa)
names(post_mean_mu) <- names(post_mean_kappa) <- levels(batting$PriPos)
batting$sim_theta <- rbeta(nrow(batting),
                           shape1=post_mean_mu[batting$PriPos] * post_mean_kappa[batting$PriPos],
                           shape2=(1-post_mean_mu[batting$PriPos]) * post_mean_kappa[batting$PriPos])
batting$sim_theta_hits <- rbinom(nrow(batting), 
                                 size=batting$AtBats, 
                                 prob=batting$sim_theta)
```

## Next: use simulated data to infer parameters, and compare to the now-known truth.




## Your turn

1. Simulate data.

2. Run Stan (just do `iter=1000`).

3. Look at the results:

    * `stan_trace()`
    * `stan_hist()`
    * `extract()`



## General questions with simulated data

1. Does my statistical inference method work?

. . .


2. Do the credible intervals contain the true value? 

    (i.e., Is the method "well-calibrated"?)

    *They usually should.*

. . .

3. How wide are credible intervals, typically?

    This is (one kind of) **statistical power.** 


# Posterior predictive sampling

## How to choose parameter values for simulation?

It depends, but one good choice is
*from the posterior distribution* -
then you can see how the model behaves
with data close to the real data.

This is known as **posterior predictive sampling**.

---------------------------

![Kruschke figure 13.2: posterior predictive sampilng](post_pred_kruschke.png)


# Sharing power // Shrinkage

## Example

Suppose that I have a large pot of coins
that are all similar to each other.
I flip each one ten times, and record the number of Heads.
What is *each coin's* probability of Heads?

- Treated *separately*,
  we would be very uncertain about each coin.

- Together, they should tell us very accurately 
  what are *likely* values of $\theta$.

- This information can improve the estimate of each separate $\theta$.

- The more similar the coins are, the more information we gain.



## Shrinkage and baseball

Some players were at bat very few times.
How does the information about their position affect our inference about their batting averages?

```{r baseball_shrinkage}
library(matrixStats)  # for colMedians etc
batting$post_med <- colMedians(param_samples$theta)
batting$post_Q1 <- colQuantiles(param_samples$theta, probs=0.25)
batting$post_Q3 <- colQuantiles(param_samples$theta, probs=0.75)
pos_means <- colMeans(param_samples$mu)
names(pos_means) <- levels(batting$PriPos)
pos_means
```

--------------

Pitchers had posterior mean $\mu$ of `r pos_means["Pitcher"]`

````{r pitchers}
with(subset(batting[order(batting$post_med),], PriPos=="Pitcher"), {
     plot(Hits / AtBats, main="Pitchers", xaxt='n', ylim=c(0, 0.4),
         xlab='player', ylab="posterior median theta");
     segments(x0=seq_along(Hits), y0=post_Q1, y1=post_Q3,
         col=adjustcolor('red',0.5));
     points(post_med, pch=20) })
abline(h=pos_means["Pitcher"], lwd=2, col=adjustcolor("blue", 0.5))
```

---------------

Catchers had posterior mean $\mu$ of `r pos_means["Catcher"]`

````{r catchers}
with(subset(batting[order(batting$post_med),], PriPos=="Catcher"), {
     plot(Hits / AtBats, main="Catchers", xaxt='n', ylim=c(0, 0.4),
         xlab='player', ylab="posterior median theta");
     segments(x0=seq_along(Hits), y0=post_Q1, y1=post_Q3,
         col=adjustcolor('red',0.5));
     points(post_med, pch=20) })
abline(h=pos_means["Catcher"], lwd=2, col=adjustcolor("blue", 0.5))
```

## Is shrinkage here a good idea?

With *simulated data*, compare median absolute error for

- posterior median $\theta_i$

- empirical batting average
