---
title: "Metric data: robustness and differences of 'means'"
author: "Peter Ralph"
date: "5 February 2018 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 4
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
set.seed(23)
library(lars)
library(tidyverse)
library(rstan)
library(matrixStats)
options(mc.cores = parallel::detectCores())
```

# Overview

## Summary

So far we've focused on *discrete* data:
coin flips and counts.

While doing that we came across familiar things,
like Poisson regression.

This week we'll look at *continuous* (i.e., "metric") data.
Different sorts of *predictors* will lead us to different classical statistics:
linear regression, $t$-tests, etcetera.

But, control of the underlying model
will let us easily get more sophisticated,
including for instance
robustness to error, and penalization for overdetermined problems.


# The basic ingredients

*Fact:* standard linear regression
is a maximum likelihood estimate 
for $b_0$ and $b_1$ under the following model:
$$\begin{aligned}
    Y_i &= b_0 + b_1 X_1 + \epsilon_i \\
    \epsilon_i &\sim \Normal(0, \sigma) .
\end{aligned}$$


## Comparison of means

If the predictor, $X$, is *discrete*
then we are doing a $t$-test, or ANOVA, or something.

. . .

Simulate data - difference in means of 3.0:

```{r simdata_t}
truth <- list(b0=1.0, b1=3.0, sigma=0.5)
n <- 200
x <- sample(c(0, +1), size=n, replace=TRUE)
y <- truth$b0 + truth$b1 * x + rnorm(n, mean=0, sd=truth$sigma)
```

------------------

The $t$-test
```{r tt}
system.time( tt <- t.test(y ~ x) )
tt
```

-----------------------------

with Stan
```{r stantt, cache=TRUE}
stt_block <- "
data { 
    int N;
    vector[N] x; // will be a vector of 0's and 1's
    vector[N] y;
}
parameters {
    real b0;
    real b1;
    real<lower=0> sigma;
}
model {
    y ~ normal(b0 + b1*x, sigma);
}"
system.time( stantt <- stan(model_code=stt_block,
                            data=list(N=length(x), x=x, y=y), iter=1e3) )
```

--------------------------

```{r summary_stantt}
rstan::summary(stantt)
```


## Standard linear regression

:::::::::::::: {.columns}
::: {.column width="50%"}


Simulate data

```r
truth <- list(b0=1.0, b1=3.0, sigma=0.5)
n <- 200
x <- rnorm(n, mean=0, sd=3)
y <- ( truth$b0 + truth$b1 * x 
        + rnorm(n, mean=0, sd=truth$sigma) )
```

:::
::: {.column width="50%"}


```{r simdata, fig.width=1.5*fig.dim, fig.height=1.5*fig.dim, echo=FALSE}
truth <- list(b0=1.0, b1=3.0, sigma=0.5)
n <- 200
x <- rnorm(n, mean=0, sd=3)
y <- truth$b0 + truth$b1 * x + rnorm(n, mean=0, sd=truth$sigma)
plot(x,y)
abline(truth$b0, truth$b1, col='red')
```

:::
::::::::::::::



------------------

Standard linear regression
```{r slr}
system.time( slr <- lm(y ~ x) )
summary(slr)
```

-----------------

with Stan
```{r stanlr, cache=TRUE}
slr_block <- "
data {
    int N;
    vector[N] x;
    vector[N] y;
}
parameters {
    real b0;
    real b1;
    real<lower=0> sigma;
}
model {
    y ~ normal(b0 + b1*x, sigma);
}"
system.time( stanlr <- stan(model_code=slr_block,
                            data=list(N=length(x), x=x, y=y), iter=1e3) )
```

----------------


```{r summary_stanlr}
rstan::summary(stanlr)
```


## Multivariate linear regrssion

Simulate data:

```{r simdata_mv}
truth <- list(b0=1.0, 
              b=c(3.0, -1.0, 0.0, 0.0),
              sigma=0.5)
n <- 200
x <- matrix(rnorm(4*n, mean=0, sd=3), ncol=4)
y <- truth$b0 + x %*% truth$b + rnorm(n, mean=0, sd=truth$sigma)
```

. . .

**Your turn:** compare standard multivariate regression
to Stan.

*Note:* `%*%` in R, and `*` in Stan, are *matrix multiplication*.

------------------

Standard linear regression
```{r mlr}
system.time( mlr <- lm(y ~ x) )
summary(mlr)
```

-----------------

with Stan
```{r stanmlr, cache=TRUE}

mlr_block <- "
data {
    int N;
    matrix[N,4] x;
    vector[N] y;
}
parameters {
    real b0;
    vector[4] b1;
    real<lower=0> sigma;
}
model {
    y ~ normal(to_vector(b0 + x * b1), sigma);
    b0 ~ normal(0, 10);
    b1 ~ normal(0, 10);
    sigma ~ normal(0, 10);
}"
system.time( stanmlr <- stan(model_code=mlr_block,
                            data=list(N=nrow(x), x=x, 
                                      y=as.vector(y)), iter=1e3) )

```

----------------


```{r summary_stanmlr}
rstan::summary(stanmlr)
```



# Testing with crossvalidation

## Crossvalidation

Just because a model *fits* doesn't mean that it's any *good*.

. . .

1. Divide your data randomly into 5 pieces.

2. Fit your model on 4/5ths, and see how well it predicts the remaining 1/5th.

3. Do this for each of the 5 pieces.

A better model should have better *crossvalidation accuracy*.

. . .

**Question:** for linear regression,
how do we "see how well it predicts"?
Write down the math, then code it up.


-----------------

```{r do_crossval}
five_groups <- sample(rep(1:5, each=nrow(x)/5))

do_crossval <- function (k) {
    # fit using not group k
    subset_lm <- lm( y ~ x, subset=(five_groups != k))
    # predict on group k
    yhat <- predict(subset_lm, 
                    newdata=list(x=x[five_groups == k,]))
    # compute crossvalidation accuracy
    S <- sqrt( mean( ( y[five_groups == k] - yhat  )^2 ) )
    return(S)
}

crossvals <- sapply(1:5, do_crossval)
```
The mean crossvalidation score for ordinary linear regression
was `r mean(crossvals)`,
with a standard deviation of `r sd(crossvals)`.



## What is overfitting?

Even completely independent sets of numbers
will correlate a little, because of noise.

. . .

When you have a *lot* of variables,
there may be some that correlate well with the response variable just by chance.


. . .

If you have as many variables than observations,
then there is (almost) *always* a linear model that fits *perfectly* (with $\epsilon = 0$).


----------------

*Example:*

```{r overparameterized}
nvars <- 200
truth <- list(b0=1.0, 
              b=rep(0.0, nvars),
              sigma=0.5)
n <- 200
x <- matrix(rnorm(nvars*n, mean=0, sd=3), ncol=nvars)
y <- truth$b0 + x %*% truth$b + rnorm(n, mean=0, sd=truth$sigma)
the_lm <- lm(y ~ x)
range(predict(the_lm) - y)
```



# Stochastic minute

---------------

If $X \sim \Cauchy(\text{center}=\mu, \text{scale}=\sigma)$, then $X$ has probability density
$$\begin{aligned}
    f(x \given \mu, \sigma) = \frac{1}{\pi\left( 1 + \left( \frac{x - \mu}{\sigma} \right)^2 \right)} .
\end{aligned}$$

> 1. The Cauchy is a good example of a distribution with "heavy tails":
>    rare, very large values.
> 
> 2. If $Y$ and $Z$ are independent $\Normal(0,1)$ then $Y/Z \sim \Cauchy(0,1)$.
>
> 3. If $X_1, X_2, \ldots, X_n$ are independent $\Cauchy(0,1)$ then
>    $\max(X_1, \ldots, X_n)$ is of size $n$.
> 
> 4. Also, $\frac{1}{n}(X_1 + \ldots + X_n) \sim \Cauchy(0,1)$.

----------------

4. If $X_1, X_2, \ldots, X_n$ are independent $\Cauchy(0,1)$ then
   $$\begin{aligned}
    \frac{1}{n} \left(X_1 + \cdots + X_n\right) \sim \Cauchy(0,1) .
   \end{aligned}$$

*Wait, what?!?*

. . .

A single value has the *same distribution* as the mean of 1,000 of them?

. . .

Let's look:
```{r cauchy_mean}
meanplot <- function (rf, n=1e3, m=100) {
    x <- matrix(rf(n*m), ncol=m)
    layout(t(1:2))
    hist(x[1,][abs(x[1,])<5], breaks=20, freq=FALSE,
         main=sprintf("%d samples", m),
         xlim=c(-5,5))
    hist(colMeans(x)[abs(colMeans(x))<5], breaks=20, freq=FALSE,
         main=sprintf("%d means of %d each", m, n),
         xlim=c(-5,5))
}
```

----------

$X \sim \Normal(0,1)$
```{r normmeans}
meanplot(rnorm)
```

-----------

$X \sim \Cauchy(0,1)$
```{r cauchymeans}
meanplot(rcauchy)
```

## Another way to look at it: extreme values

```{r max_values}
n <- 100
plot(c(cummax(rcauchy(n))), type='l', ylab='max value so far', xlab='number of samples', col='red')
lines(c(cummax(rnorm(n))), col='black')
legend("bottomright", lty=1, col=c('black', 'red'), legend=c('normal', 'cauchy'))
```

## Another way to look at it: extreme values

```{r max_values2}
n <- 1000
plot(c(cummax(rcauchy(n))), type='l', ylab='max value so far', xlab='number of samples', col='red')
lines(c(cummax(rnorm(n))), col='black')
legend("bottomright", lty=1, col=c('black', 'red'), legend=c('normal', 'cauchy'))
```

## Another way to look at it: extreme values

```{r max_values3}
n <- 1e6
plot(c(cummax(rcauchy(n))), type='l', ylab='max value so far', xlab='number of samples', col='red')
lines(c(cummax(rnorm(n))), col='black')
legend("bottomright", lty=1, col=c('black', 'red'), legend=c('normal', 'cauchy'))
```



# Problem #1: noise

## Cauchy noise

Let's see what happens if 
$$\begin{aligned}
    Y_i &= b_0 + b_1 X_1 + \epsilon_i \\
    \epsilon_i &\sim \Cauchy(0, \sigma) .
\end{aligned}$$

. . .

```{r simdata_c}
truth <- list(b0=1.0, b1=3.0, sigma=0.5)
n <- 1000
x <- sample(c(0, +1), size=n, replace=TRUE)
y <- truth$b0 + truth$b1 * x + rcauchy(n, location=0, scale=truth$sigma)
```

------------------

The $t$-test
```{r ct}
system.time( tt <- t.test(y ~ x) )
tt
```

-----------------------------

with Stan
```{r stantct, cache=TRUE}
sct_block <- "
data { 
    int N;
    vector[N] x; // will be a vector of 0's and 1's
    vector[N] y;
}
parameters {
    real b0;
    real b1;
    real<lower=0> sigma;
}
model { 
    y ~ cauchy(b0 + b1*x, sigma);
}"
system.time( stanct <- stan(model_code=sct_block,
                            data=list(N=length(x), x=x, y=y), iter=1e3) )
```

--------------------------

```{r summary_stanct}
rstan::summary(stanct)
```

-----------

Because we *correctly model* the noise,
using Stan,
we are not thrown off by large values.



# Robust ANOVA

## Metric data from groups

Suppose we have numerical observations coming from $m$ different groups,
and want to know if the *means* are different between groups, and by how much.

. . .

:::::::::::::: {.columns}
::: {.column width="50%"}

e.g., dry leaf mass after 10d of growth
in 6 different conditions


:::
::: {.column width="50%"}


```{r simdata_ra, fig.width=1.5*fig.dim, fig.height=1.5*fig.dim}
m <- 20
truth <- list(b=200 + rnorm(m, sd=5), 
              sigma=2.0)
n <- 200
x <- sample(1:m, size=n, replace=TRUE)
y <- pmax(0, truth$b[x]
              + rcauchy(n, location=0, 
                      scale=truth$sigma) )
```


:::
::::::::::::::


-----------------


:::::::::::::: {.columns}
::: {.column width="50%"}

dry leaf mass after 10d of growth
in 6 different conditions

```r
boxplot(y ~ x, main='all data')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
boxplot(y ~ x, subset=abs(y-200)<20, xlab='treatment', main='abs(y-200) < 10')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
```


:::
::: {.column width="50%"}

```{r plotdata_ra, fig.width=1.5*fig.dim, fig.height=2*fig.dim, echo=FALSE}
layout(1:2)
boxplot(y ~ x, main='all data')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
boxplot(y ~ x, subset=abs(y-200)<20, xlab='treatment', main='abs(y-200) < 10')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
```

:::
::::::::::::::


-------------------


:::::::::::::: {.columns}
::: {.column width="50%"}

Each group has a different location and scale;
noise about these is Cauchy;
group locations are random deviations;
group scale parameters are random deviations from a common scale.


$$\begin{aligned}
    Y_i &\sim \Cauchy(b_{g_i}, \sigma_{g_i}) \\
    \sigma_g &\sim \log\Normal(\mu, 1) \\
    b_g &\sim \Normal(0, \eta) \\
    \eta &\sim \Normal(0, 15) \\
    \mu &\sim \Normal(0, 1)
\end{aligned}$$

:::
::: {.column width="50%"}

```{r plotdata_ra4, fig.width=1.5*fig.dim, fig.height=2*fig.dim, echo=FALSE}
layout(1:2)
boxplot(y ~ x, main='all data')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
boxplot(y ~ x, subset=abs(y-200)<20, xlab='treatment', main='abs(y-200) < 10')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
```

:::
::::::::::::::


-------------------


:::::::::::::: {.columns}
::: {.column width="50%"}

$$\begin{aligned}
    Y_i &\sim \Cauchy(b_{g_i}, \sigma_{g_i}) \\
    \sigma_g &\sim \log\Normal(\mu, 1) \\
    b_g &\sim \Normal(z, \eta) \\
    z &\sim \Normal(0, 10) \\
    \eta &\sim \Normal(0, 15) \\
    \mu &\sim \Exp(1)
\end{aligned}$$

*Changes:*

1. Added common mean, $z$.

2. Put "shrinkage" prior on $\mu$.

:::
::: {.column width="50%"}

```{r plotdata_ra3, fig.width=1.5*fig.dim, fig.height=2*fig.dim, echo=FALSE}
layout(1:2)
boxplot(y ~ x, main='all data')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
boxplot(y ~ x, subset=abs(y-200)<20, xlab='treatment', main='abs(y-200) < 10')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
```

:::
::::::::::::::

-------------------


:::::::::::::: {.columns}
::: {.column width="50%"}


```{r do_anova, cache=TRUE}
anova_block <- "
data {
    int N; // number of obs
    int m; // number of groups
    vector[N] y; // leaf mass
    int g[N]; // group ID
}
parameters {
    vector[m] b;  // group 'means'
    real z;  // overall mean
    // group 'SD's
    vector<lower=0>[m] sigma; 
    // variability in group means
    real<lower=0> eta;  
    // log-mean of sigma
    real mu; 
}
model {
    y ~ cauchy(b[g], sigma[g]);
    // lognormal, encouraging all group sigmas to be the same,
    // but trying not to constrain them if they want to be different
    sigma ~ lognormal(mu, 1);
    // unsure of scale for b, 
    // so put a hyperprior on it
    b ~ normal(z, eta);
    z ~ normal(0, 15);
    eta ~ normal(0, 15);
    // encourage mu to be near zero if reasonable
    mu ~ exponential(1);
}"

## note we subtract the median of y
anova_data <- list(N=length(y),
                   m=length(unique(x)),
                   y=y - median(y),
                   g=x)

anova_fit <- stan(model_code=anova_block,
                  data=anova_data,
                  iter=1e3,
                  control=list(adapt_delta=0.99))
```

:::
::: {.column width="50%"}

```{r plotdata_ra2, fig.width=1.5*fig.dim, fig.height=2*fig.dim, echo=FALSE}
layout(1:2)
boxplot(y ~ x, main='all data')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
boxplot(y ~ x, subset=abs(y-200)<20, xlab='treatment', main='abs(y-200) < 10')
points(seq_along(truth$b), truth$b, pch=20, cex=2.0, col='red')
```

:::
::::::::::::::

--------------

```{r anova_run_summary}
rstan::summary(anova_fit)
```

--------------

:::::::::::::: {.columns}
::: {.column width="50%"}


*What do we want to know?*

1. Do the treatments affect mean growth?

2. How much does leaf mass in treatment 8 differ from group 12?

3. How much variation is explained by treatment?

:::
::: {.column width="50%"}


*How do we find it out?*

1. See if any credible intervals overlap?
   And, see if credible interval for eta > 0.

2. Posterior distribution of `b[12] - b[8]`: if `>0` then `b[12] > b[8]`; if zero is in it then `b[12] = b[8]`.

3. Compare `sigma` to `eta`.
   Or take the ratio of MADs before and after subtracting `b[-]`.

:::
::::::::::::::


## Do treatments affect growth?


## How much do treatments 8 and 12 differ?


## How much variation is explained by treatment?


