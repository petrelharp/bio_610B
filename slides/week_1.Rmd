---
title: "Prior distributions and uncertainty"
author: "Peter Ralph"
date: "8 January 2018 -- Advanced Biological Statistics"
---

```{r setup, include=FALSE}
fig.dim <- 3
knitr::opts_chunk$set(fig.width=2*fig.dim,
                      fig.height=fig.dim,
                      fig.align='center')
```

## Today

0. Course structure and introductions

    * short survey

1. Probability: review and notation

2. Bayesian analysis

3. Application: Beta distributed coins


# Biased coins

## a motivating example

Suppose I have two trick coins:

* one (coin A) comes up heads 75% of the time, and 
* the other (coin B) only 25% of the time.

. . .

But, I lost one and I don't know which!
So, I flip it **10 times** and get **6 heads**.
*Which is it, and how sure are you?*

--------------

## Possible answers:

> 1. Er, probably coin #1?
> 
> 2. Well,
>    $$\begin{aligned}
>    \P\{ \text{6 H in 10 flips} \given \text{coin A} \}
>    &= \binom{10}{6} (.75)^6 (.25)^4 \\
>    &= 0.146
>    \end{aligned}$$
>    and
>    $$\begin{aligned}
>    \P\{ \text{6 H in 10 flips} \given \text{coin B} \}
>    &= \binom{10}{6} (.25)^6 (.75)^4 \\
>    &= 0.016
>    \end{aligned}$$
>    ... so, probably coin A?

---------------

For a precise answer...

3. *Before flipping*, each coin seems equally likely.  Then

    $$\begin{aligned}
    \P\{ \text{coin A} \given \text{6 H in 10 flips} \}
    &= \frac{
    \frac{1}{2} \times 0.146
    }{
    \frac{1}{2} \times 0.146
    +
    \frac{1}{2} \times 0.016
    } \\
    &= 0.9
    \end{aligned}$$



# Probability: review and notation

## Probability rules:

> 0. *Probabilities are proportions:* $\hspace{2em} 0 \le \P\{A\} \le 1$
> 
> 1. *Everything:* $\hspace{2em} \P\{ \Omega \} = 1$
> 
> 2. *Complements:* $\hspace{2em} \P\{ \text{not } A\} = 1 - \P\{A\}$
> 
> 3. *Disjoint events:* If $\hspace{2em} \P\{A \text{ and } B\} = 0$ then $\hspace{2em} \P\{A \text{ or } B\} = \P\{A\} + \P\{B\}$.
> 
> 4. *Independence:* $A$ and $B$ are independent iff $\P\{A \text{ and } B\} = \P\{A\} \P\{B\}$.
> 
> 5. *Conditional probability:* 
>     $$\P\{A \given B\} = \frac{\P\{A \text{ and } B\}}{ \P\{B\} }$$

---------------

## Bayes' rule

A consequence is

$$\P\{B \given A\} = \frac{\P\{B\} \P\{A \given B\}}{ \P\{A\} } .$$

. . .

In "Bayesian statistics":

> - $B$: possible model
> - $A$: data
> - $\P\{B\}$: prior weight on model $B$
> - $\P\{A \given B\}$: likelihood of data under $B$
> - $\P\{B\} \P\{A \given B\}$: posterior weight on $B$
> - $\P\{A\}$: total sum of posterior weights


# Group problem

## More coins

Suppose instead I had 9 coins, with probabilities 10%, 20%, ..., 80%, 90%;
as before I flipped on 10 times and got 6 heads.
For each $\theta$ in $0.1, 0.2, \ldots, 0.8, 0.9$, find
$$\begin{aligned}
    \P\{\text{ coin has prob $\theta$ } \given \text{ 6 H in 10 flips } \}
\end{aligned}$$
and plot these as a bar plot.

*Question:* which coin(s) is it, and how sure are you?
(and, what do you mean when you say how sure you are)

*Time allowing,* do it again with 99 coins. And 999 coins. Does your answer change?




# Breaking it down


## Uniform prior

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}
```{r the_prior, echo=FALSE, fig.height=2.5*fig.dim}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(6, size=10, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```
:::
::::::::::::::

----------------------------

## Weak prior

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}
```{r weak_prior, echo=FALSE, fig.height=2.5*fig.dim}
theta <- (1:9)/10
prior <- (9:1)/45
likelihood <- dbinom(6, size=10, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```
:::
::::::::::::::

----------------------------

## Strong prior

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}
```{r strong_prior, echo=FALSE, fig.height=2.5*fig.dim}
theta <- (1:9)/10
prior <- 2^(8:0)/511
likelihood <- dbinom(6, size=10, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```
:::
::::::::::::::


## The likelihod: 6 H in 10 flips

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}
```{r ten_flips, echo=FALSE, fig.height=2.5*fig.dim}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(6, size=10, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```
:::
::::::::::::::

----------------

## The likelihod: 30 H in 50 flips

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}
```{r fifty_flips, echo=FALSE, fig.height=2.5*fig.dim}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(30, size=50, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```
:::
::::::::::::::


----------------

## The likelihod: 60 H in 100 flips

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}
```{r 100_flips, echo=FALSE, fig.height=2.5*fig.dim}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(60, size=100, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```
:::
::::::::::::::

----------------

## The likelihod: 6,000 H in 10,000 flips

:::::::::::::: {.columns}
::: {.column width="20%"}

prior

$\times$

likelihood

$\propto$

posterior


:::
::: {.column width="80%"}
```{r ten_thou_flips, echo=FALSE, fig.height=2.5*fig.dim}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(6000, size=10000, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
names(posterior) <- theta
layout(1:3)
par(mar=c(4,4,1.5,1)+.1, mgp=c(2.6,0.8,0))
plot(theta, prior, type='b'); title("prior", line=0.5)
plot(theta, likelihood, type='b'); title("likelihood", line=0.5)
plot(theta, posterior, type='b'); title("posterior", line=0.5)
```
:::
::::::::::::::


# A question

## What is the right answer to the "coin question"?

:::::::::::::: {.columns}
::: {.column width="60%"}

Recall: there were ten possible values of $\theta$.


Which coin is it, and how sure are you?


*Possible types of answer:*

1. "best guess"
2. "range of values"
3. "don't know"


Give examples of when each type of answer is the right one.

:::
::: {.column width="40%"}


```{r plot_pos}
theta <- (1:9)/10
prior <- rep(1/9, 9)
likelihood <- dbinom(6, size=10, prob=theta)
posterior <- prior*likelihood/sum(prior*likelihood)
names(posterior) <- theta
barplot(posterior, xlab='true prob of heads')
```

:::
::::::::::::::



# Stochastic Minute

## The Beta Distribution

If $$P \sim \text{Beta}(a,b)$$
then $P$ has *probability density*
$$p(\theta) = \frac{ \theta^{a-1} (1 - \theta)^{b-1} }{ B(a,b) } . $$

- Takes values between 0 and 1.

- If $U_{(1)} < U_{(2)} < \cdots < U_{(n)}$ are sorted, independent $\text{Unif}[0,1]$
  then $U_{(k)} \sim \text{Beta}(k, n-k)$.

- Mean: $a/(a+b)$.

- Larger $a+b$ is more tightly concentrated (like $1/\sqrt{a+b}$)


# Unknown coins

## Motivating example

Now suppose we want to estimate the probability of heads
for a coin *without* knowing the possible values.
(or, a disease incidence, or error rate in an experiment, ...)

We flip it $n$ times and get $z$ Heads.

The *likelihood* of this, given the prob-of-heads $\theta$, is:
$$p(z \given \theta) = \binom{n}{z}\theta^z (1-\theta)^{n-z} . $$

How to weight the possible $\theta$?
Need a flexible set of weighting functions, i.e.,
**prior distributions** on $[0,1]$.

. . .

* **Beta** distributions.

--------------

What would we use if:

- the coin is probably close to fair.

- the disease is probably quite rare.

- no idea whatsoever.


## Beta-Binomial Bayesian analysis

If
$$\begin{aligned}
P &\sim \text{Beta}(a,b) \\
Z &\sim \text{Binom}(n,P) ,
\end{aligned}$$
then by Bayes' rule:
$$\begin{aligned}
 \P\{ P = \theta \given Z = z\}
 &=
 \frac{\P\{Z = z \given P = \theta \} \P\{P = \theta\}}{\P\{Z = z\}} \\
 &= 
 \frac{
 \binom{n}{z}\theta^z (1-\theta)^{n-z}
 \times
 \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)}
 }{
 \text{(something)}
 } \\
 &=
 \text{(something else)} \times \theta^{a + z - 1} (1-\theta)^{b + n - z - 1} .
\end{aligned}$$

. . .

Miraculously,
$$\begin{aligned}
P \given Z = z \sim \text{Beta}(a+z, b+n-z) .
\end{aligned}$$


## Exercise: check this.

1. Simulate $10^6$ coin *probabilities*, called $\theta$, from Beta(5,5). (`rbeta`)

2. For *each coin*, simulate 10 flips. (`rbinom`)

3. Make a histogram of the true probabilities (values of $\theta$)
   of *only* those coins having 3 of 10 heads.

4. Compare the distribution to Beta($a$,$b$) -- with what $a$ and $b$? (`dbeta`)

5. Explain what happened.


# Beta-binomial: example analysis

## Discuss/demonstration:

We flip an odd-looking coin 100 times,
and get 65 heads.
What is it's true* probability of heads?

1. True = ??

2. What prior to use?

3. Is it reasonable that $\theta = 1/2$?

4. Best guess at $\theta$?

5. How far off are we, probably?

6. How much does the answer depend on the prior?

7. Does our procedure work on simulated data?

# Reporting uncertainty

## Credible region

**Definition:** A 95% *credible region* is a portion of parameter space
having a total of 95% of the *posterior probability*.

. . .

(same with other numbers for "95%")

## Interpretation \#1

If we construct a 95% credible interval for $\theta$
for each of many datasets;
*and* the coin in each dataset has $\theta$ drawn independently from the prior,
*then* the true $\theta$ will fall in its credible interval for 95% of the datasets.


## Interpretation \#2

If we construct a 95% credible interval for $\theta$ with a dataset,
and the distribution of the coin's true $\theta$ across many parallel universes
is given by the prior,
then the true $\theta$ will be in the credible interval
in 95% of those universes.



## Interpretation \#3

Given my prior beliefs (prior distribution),
the posterior distribution is the most rational${}^*$ 
way to update my beliefs to account for the data.

. . .

${}^*$ if you do this many times you will be wrong least often

. . .

${}^*$ **or** you will be wrong in the fewest possible universes


## But which credible interval?

**Definition:** The "95\% highest density interval" is the 95\% credible interval
with the highest posterior probability.

```{r hdi_plot}
# plot
```

# The Central Limit Theorem

## How many flips to know within $\approx$ 1\%?

:::::::::::::: {.columns}
::: {.column width="60%"}


Flip a $\theta$-coin $n$ times, get $X$ heads, so

$$ X \sim \text{Binom}(n, \theta) $$

and if $P = X/n$, then

$$ \sd[P] = \sqrt{\frac{\theta(1-\theta)}{n}} . $$


**Also:**
$$\begin{aligned}
    P &\approx \Normal(\theta, \sd[P]) .
\end{aligned}$$


*(discussion)*

:::
::: {.column width="40%"}

```{r plot_clt, echo=FALSE, fig.height=2*fig.dim, fig.width=1.5*fig.dim}
layout(1:3)
par(mar=c(2,2,4,1)+.1)
xh <- hist(rbinom(1e6, size=100, prob=3/5)/100, breaks=((0:101)-0.5)/100, freq=FALSE,
           main='100 flips', xlim=c(0,1), xlab='')
lines(xh$mids, diff(pnorm(q=xh$breaks, mean=3/5, sd=sqrt(3*2)/50))/diff(xh$breaks),
      col=adjustcolor('red',0.5), lwd=2)
xh <- hist(rbinom(1e6, size=1e3, prob=3/5)/1e3, breaks=100, freq=FALSE,
           main='1,000 flips', xlim=c(0,1), xlab='')
lines(xh$mids, diff(pnorm(q=xh$breaks, mean=3/5, sd=sqrt(3*2/1e3)/5))/diff(xh$breaks),
      col=adjustcolor('red',0.5), lwd=2)
xh <- hist(rbinom(1e6, size=1e4, prob=3/5)/1e4, breaks=100, freq=FALSE,
           main='10,000 flips', xlim=c(0,1), xlab='')
lines(xh$mids, diff(pnorm(q=xh$breaks, mean=3/5, sd=sqrt(3*2/1e4)/5))/diff(xh$breaks),
      col=adjustcolor('red',0.5), lwd=2)
```

:::
::::::::::::::


