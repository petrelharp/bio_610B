<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Peter Ralph" />
  <meta name="dcterms.date" content="2018-01-09" />
  <title>Schedule :: Advanced Biological Statistics II: Bio 610, Winter 2018</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  \[
  %%
  % Add your macros here; they'll be included in pdf and html output.
  %%
  
  \newcommand{\R}{\mathbb{R}}    % reals
  \newcommand{\E}{\mathbb{E}}    % expectation
  \renewcommand{\P}{\mathbb{P}}  % probability
  
  \newcommand{\given}{\;\vert\;}
  \]
</head>
<body>
<header>
<h1 class="title">Schedule :: Advanced Biological Statistics II: Bio 610, Winter 2018</h1>
<p class="author">Peter Ralph</p>
<p class="date">9 January 2018</p>
</header>
<h1 id="course-schedule">Course schedule</h1>
<p>The tentative schedule (subject to adjustment, especially towards the end) is (<strong>K</strong> referes to Kruschke):</p>
<dl>
<dt>Week 1 (<em>1/9</em>)</dt>
<dd><p><a href="slides/week_1.slides.html">(slides)</a> Recap of probability and likelihood; central limit theorem (<span class="math inline">\(\sqrt{n}\)</span>); Bayes’ rule. The beta-binomial distribution: putting a prior on the probability of success. <strong>(K ch. 4, 5, 6)</strong></p>
<p><strong>Short report:</strong> (due 1/16) <a href="hws/week_1.html">MLE for beta-binomial</a></p>
</dd>
<dt>Week 2 (<em>1/16</em>)</dt>
<dd><p>Introduction to MCMC and Stan for sampling from posterior distributions, hierarchical models for binary responses, shrinkage. <strong>(K ch. 7, 9 and Intro to Stan)</strong></p>
</dd>
<dt>Week 3 (<em>1/23</em>)</dt>
<dd><p>Assessing power, model choice, and using simulation: looking more at shrinkage, posterior predictive sampling, Bayes factors. Logistic regression: robustly, including categorical factors. <strong>(K ch 13 and 21, with a bit of chapters 10-12)</strong></p>
</dd>
<dt>Week 4 (<em>1/30</em>)</dt>
<dd><p>Count data: using Poisson regression and hierarchical modeling to fit overdispersion. Model selection by crossvalidation. <strong>(K ch 24)</strong></p>
</dd>
<dt>Week 5 (<em>2/6</em>)</dt>
<dd><p>Continuous (“metric”) data: groupwise means, univariate regression, robust regression by adjusting the noise distribution, multivariate regression and variable selection; the Horseshoe. <strong>(K ch 16, 17, 18)</strong></p>
</dd>
<dt>Week 6 (<em>2/13</em>)</dt>
<dd><p>Friends of ANOVA: sources of variance. An in-depth applied example, cumulative. <strong>(K ch 19, 20)</strong></p>
</dd>
<dt>Week 7 <em>(2/20)</em></dt>
<dd><p>Dimension reduction: overview of the goal and taxonomy of various approaches (PCA, CCA, t-sne, etc). Deconvolution. Regularization as a penalty and as a (Bayesian) prior. <strong>(Reference to Quinn &amp; Keough)</strong></p>
</dd>
<dt>Week 8 <em>(2/27)</em></dt>
<dd><p>Clustering and categorization: softmax regression; applied example. <strong>(K ch 22)</strong></p>
</dd>
<dt>Week 9 <em>(3/6)</em></dt>
<dd><p>Time series: modeling local dependency, smoothing. Conditional independence.</p>
</dd>
<dt>Week 10 <em>(3/13)</em></dt>
<dd><p>Spatial and network covariance: sharing power between related locations.</p>
</dd>
</dl>
</body>
</html>
